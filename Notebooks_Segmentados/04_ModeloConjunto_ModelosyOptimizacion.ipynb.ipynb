{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec91e29-59b9-4760-a40d-7cfc86634f94",
   "metadata": {},
   "source": [
    "# Modelo Conjunto GrandSlams + Masters1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4b1d4-978a-4bb9-927a-864fe876e042",
   "metadata": {},
   "source": [
    "## Modelo Regresión Logistica (GS + M1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f569ca-ad40-41cb-9776-b4b9bceb881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a escalar\n",
    "features_to_scale = [\n",
    "    \"Surface_WinRate_Favorite\", \"Surface_WinRate_Not_Favorite\",\n",
    "    \"Surface_Matches_Favorite\", \"Surface_Matches_Not_Favorite\",\n",
    "    \"WinStreak_Favorite\", \"WinStreak_Not_Favorite\", \"Win_Streak_Diff\",\n",
    "    \"Rank_Favorite\", \"Rank_Not_Favorite\", \"Rank_Diff_Signed\", \"Rank_Diff_Abs\",\n",
    "    \"GrandSlams_Favorite\", \"GrandSlams_Not_Favorite\",\n",
    "    \"Masters1000_Favorite\",\"Masters1000_Not_Favorite\"\n",
    "]\n",
    "\n",
    "# Asegurarse de que son numéricos\n",
    "for col in features_to_scale:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Escalado con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d662e-e782-4db4-a73d-190fe8b1086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar los valores nulos por columna para comprobar que no haya nulos\n",
    "print(df[features_to_scale + [\"Favorite_Wins\"]].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf2bea-8eea-4e4c-80bf-576921bd0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features_to_scale]  # Asegurarse de que no haya columnas extra\n",
    "y = df[\"Favorite_Wins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5d245-1859-49b0-bfc7-743e73acb1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos que no haya nulos en x ni en y\n",
    "print(\"¿NaNs en X?:\", X.isna().any().any()) \n",
    "print(\"¿NaNs en y?:\", y.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bf61e-7bcc-477f-a987-ee8f986587cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo de regresión logística\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular y mostrar la métrica\n",
    "score = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee53f3-2d1d-47d9-9d8b-94c235c56baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680cd01-25b5-4319-bc30-43c1f388cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo de regresión logística con ajuste por desbalance\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar con balanced accuracy\n",
    "score = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac1d5b-7630-48b7-b69b-3782eeec0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prueba con validación cruzada\n",
    "\n",
    "# Usamos solo las columnas que queremos escalar\n",
    "X = df[features_to_scale]\n",
    "y = df[\"Favorite_Wins\"]  \n",
    "\n",
    "# Modelo con validación cruzada (5 folds)\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "y_pred_cv = cross_val_predict(model, X, y, cv=5)\n",
    "\n",
    "# Evaluación\n",
    "score_cv = balanced_accuracy_score(y, y_pred_cv)\n",
    "print(\"Balanced Accuracy (CV):\", round(score_cv, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682031f-ebd0-42f5-8fa4-ab3df06f4798",
   "metadata": {},
   "source": [
    "### Evaluación del modelo Regresión Logística (GS + M1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040368c-add0-409c-980f-f38b6e5f162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones con validación cruzada\n",
    "y_proba = cross_val_predict(model, X, y, cv=5, method='predict_proba')\n",
    "y_pred = (y_proba[:, 1] >= 0.5).astype(int)  # convertir probabilidades en clases\n",
    "\n",
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee84c1d-5e54-4e6e-b107-05e7a8cb5643",
   "metadata": {},
   "source": [
    "## Random Forest + validación cruzada (GS + M1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29fa72-3b3a-42a3-8f53-4321ac51479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_rf = cross_val_predict(rf_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_rf = (y_proba_rf[:, 1] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09394fe6-3647-4f38-adf4-11a641b9d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_rf), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_rf), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_rf), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_rf), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_rf), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_rf[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_rf[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_rf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baee555-0f00-4662-8f3f-4906bf0deb5f",
   "metadata": {},
   "source": [
    "## Modelo Gradient Boosting (GS + M1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0920d0-8f03-4242-86f5-e1106c1506a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_gb = cross_val_predict(gb_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_gb = (y_proba_gb[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_gb), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_gb), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_gb), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_gb[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_gb[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_gb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b613c8b-dc2a-4008-bef8-1e0adce84ee9",
   "metadata": {},
   "source": [
    "## Modelo Gradient Boosting balanceado (GS + M1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0f061-8374-4017-95e8-1f3ea68a59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = HistGradientBoostingClassifier(class_weight='balanced', max_iter=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_gb = cross_val_predict(gb_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_gb = (y_proba_gb[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_gb), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_gb), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_gb), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_gb[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_gb[:, 1]), 3))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_gb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e4a94-5098-48a5-a415-3d87205b25b1",
   "metadata": {},
   "source": [
    "## MLPClassifier (GS + M1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13725f7-3ac3-4d7a-ad3b-e6ad86827696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo MLP\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),  # una capa oculta con 100 neuronas\n",
    "    max_iter=300,\n",
    "    alpha=1e-4,\n",
    "    solver='adam',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_mlp = cross_val_predict(mlp_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_mlp = (y_proba_mlp[:, 1] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3e33a-28aa-4680-ac69-c207ecb15435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_mlp), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_mlp), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_mlp), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_mlp[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_mlp[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_mlp))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8977f9bc-3146-48d2-aa88-4829e9de815c",
   "metadata": {},
   "source": [
    "## MLPClassifier con SMOTE (GS + M1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79604e-1429-4304-a1ae-0c35b20a5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Aplicar SMOTE al conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 3. Entrenar MLP sobre datos balanceados\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "mlp_model.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# 4. Predecir sobre el test original\n",
    "y_pred_mlp = mlp_model.predict(X_test)\n",
    "y_proba_mlp = mlp_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# 5. Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_mlp), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y_test, y_pred_mlp), 3))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred_mlp), 3))\n",
    "print(\"Recall:\", round(recall_score(y_test, y_pred_mlp), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y_test, y_pred_mlp), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y_test, y_proba_mlp[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y_test, y_proba_mlp[:, 1]), 3))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_mlp))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a1c1b-a5e9-4cdd-b234-537994457a63",
   "metadata": {},
   "source": [
    "| Modelo                    | Accuracy | Balanced Accuracy | Precision | Recall | F1-Score | AUC-ROC | Log Loss |\n",
    "|--------------------------|----------|--------------------|-----------|--------|----------|---------|----------|\n",
    "| Regresión Logística      | 0.612    | 0.615              | 0.771     | 0.608  | 0.680    | 0.657   | 0.656    |\n",
    "| Random Forest            | 0.673    | 0.548              | 0.701     | 0.901  | 0.789    | 0.646   | 0.606    |\n",
    "| Gradient Boosting        | 0.679    | 0.545              | 0.699     | 0.925  | 0.796    | **0.671** | 0.589 |\n",
    "| Gradient Boosting (bal.) | 0.608    | 0.625              | 0.787 | 0.577  | 0.666    | 0.669   | 0.640    |\n",
    "| MLPClassifier            | 0.679    | 0.555              | 0.704     | 0.906  | 0.793    | **0.666**   | 0.596    |\n",
    "| MLP + SMOTE              | 0.596    | 0.613              | 0.776     | 0.564  | 0.653    | 0.653   | 0.669    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a93479-cde9-41e9-a834-60cb1730aca0",
   "metadata": {},
   "source": [
    "## Mejora de Modelos con mas AUC-ROC : Gradient Bossting y MLPClassifier (GS + M1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096790a-5b82-4fae-b9d2-c67d22828e12",
   "metadata": {},
   "source": [
    "## Modelo Gradient Boosting con Optua (GS + M1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ded72-1e92-4b76-924d-1dfd1d333735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objetivo de optimización\n",
    "def objective(trial):\n",
    "    # Hiperparámetros a optimizar\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    # Validación cruzada\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "        model = GradientBoostingClassifier(\n",
    "            **params,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, sample_weight=weights)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "# Crear y ejecutar estudio\n",
    "study = optuna.create_study(direction='maximize', study_name='GB_AUC_Optimization')\n",
    "study.optimize(objective, n_trials=100, n_jobs=1)  # subir n_trials para mejor resultado\n",
    "\n",
    "# Mostrar mejor resultado\n",
    "print(\" Mejor AUC-ROC:\", round(study.best_value, 4))\n",
    "print(\" Mejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e009e4-323e-46e8-8cb9-177b4ffdbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    " Mejor AUC-ROC: 0.6818\n",
    " Mejores hiperparámetros encontrados:\n",
    "{'learning_rate': 0.014538386775171604, 'n_estimators': 475, 'max_depth': 4, 'subsample': 0.5162888716243986}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10bbe2-b993-4ebb-969d-1429618a6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_proba = np.zeros(len(y))\n",
    "y_pred = np.zeros(len(y))\n",
    "\n",
    "# Validación cruzada manual con los mejores hiperparámetros encontrados\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    w = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "    model = GradientBoostingClassifier(\n",
    "        learning_rate=0.014538386775171604,\n",
    "        n_estimators=475,\n",
    "        max_depth=4,\n",
    "        subsample=0.5162888716243986,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, sample_weight=w)\n",
    "    y_proba[test_idx] = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred[test_idx] = model.predict(X_test)\n",
    "\n",
    "# Evaluación\n",
    "y_pred = y_pred.astype(int)\n",
    "\n",
    "print(\"MÉTRICAS DEL MODELO FINAL (Optuna):\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba), 3))\n",
    "print(\"\\nMatriz de Confusión:\\n\", confusion_matrix(y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649912c-9a95-4790-b8c8-1f8445c930bf",
   "metadata": {},
   "source": [
    "## MLPClassifier Optua (GS + M1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e70fe6-906f-4099-af25-053f26212b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mostrar progreso en consola\n",
    "#optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# Objetivo de optimización\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50, 50), (100, 50), (100, 100)]),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-5, 1e-2, log=True),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 0.0005, 0.1, log=True),\n",
    "        'solver': trial.suggest_categorical('solver', ['adam', 'sgd']),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    }\n",
    "\n",
    "    aucs = []\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = MLPClassifier(\n",
    "            **params,\n",
    "            max_iter=300,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "# Crear estudio y optimizar\n",
    "study = optuna.create_study(direction='maximize', study_name='MLP_AUC_Optimization')\n",
    "study.optimize(objective, n_trials=150, n_jobs=1)  \n",
    "\n",
    "# Mostrar mejores resultados\n",
    "print(\"Mejor AUC-ROC:\", round(study.best_value, 4))\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7bbfb9-812b-4633-97fc-dfe7754fe87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mejor AUC-ROC: 0.6777\n",
    "Mejores hiperparámetros encontrados:\n",
    "{'hidden_layer_sizes': (100, 50), 'alpha': 0.007356732091984054, 'learning_rate_init': 0.003647090832896848, 'solver': 'sgd', 'activation': 'tanh'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a5b17-08b4-41b8-8c5d-297173c7f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modelo con los mejores hiperparámetros encontrados por Optuna\n",
    "best_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    alpha=0.007356732091984054,\n",
    "    learning_rate_init=0.003647090832896848,\n",
    "    solver='sgd',\n",
    "    activation='tanh',\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Validación cruzada (probabilidades)\n",
    "y_proba = cross_val_predict(best_mlp, X, y, cv=5, method='predict_proba')\n",
    "y_pred = (y_proba[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# 3. Métricas\n",
    "print(\"MÉTRICAS DEL MLP OPTIMIZADO (Optuna actualizado):\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba[:, 1]), 3))\n",
    "\n",
    "# 4. Matriz de confusión\n",
    "print(\"\\nMatriz de Confusión:\\n\", confusion_matrix(y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1faec-c706-4b62-a2fe-d3c3442c1d8e",
   "metadata": {},
   "source": [
    "| Modelo                    | Accuracy | Balanced Accuracy | Precision | Recall | F1-Score | AUC-ROC | Log Loss |\n",
    "|--------------------------|----------|--------------------|-----------|--------|----------|---------|----------|\n",
    "| Gradient Boosting (Optuna) | 0.613    | **0.631**          | **0.792** | 0.581  | 0.670    | **0.682** | 0.636    |\n",
    "| MLPClassifier (Optuna)     | **0.682**| 0.545              | 0.698     | **0.934** | **0.799** | 0.672   | **0.588** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b4270-eb3a-4bb7-95b4-a76cc2c89122",
   "metadata": {},
   "source": [
    "### Comparativa de los Mejores Modelos por AUC-ROC\n",
    "\n",
    "| Métrica               | Grand Slams (GB Optuna) | Masters 1000 (GB Optuna) | GS + M1000 (GB Optuna) | GS + M1000 (MLP Optuna) |\n",
    "|-----------------------|-------------------------|---------------------------|-------------------------|--------------------------|\n",
    "| **Accuracy**          | 0.643                   | 0.594                     | 0.613                   | **0.682**                |\n",
    "| **Balanced Accuracy** | **0.658**               | 0.606                     | **0.631**               | 0.545                    |\n",
    "| **Precision**         | **0.835**               | 0.743                     | **0.792**               | 0.698                    |\n",
    "| **Recall**            | 0.623                   | 0.563                     | 0.581                   | **0.934**                |\n",
    "| **F1-Score**          | 0.714                   | 0.641                     | 0.670                   | **0.799**                |\n",
    "| **AUC-ROC**           | **0.713**               | 0.649                     | **0.682**               | 0.672                    |\n",
    "| **Log Loss**          | 0.613                   | 0.654                     | 0.636                   | **0.588**                |\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión Final\n",
    "\n",
    "- El modelo **Gradient Boosting con Optuna en Grand Slams** sigue siendo el mejor en cuanto a **AUC-ROC (0.713)**, además de tener el **mejor balance entre precisión y sensibilidad**.\n",
    "- El modelo **combinado (Grand Slams + Masters 1000)** mejora el recall al usar MLP optimizado y alcanza el **mejor F1-Score y Accuracy**, aunque pierde en Balanced Accuracy y AUC.\n",
    "- El conjunto de **Masters 1000** es el más difícil de predecir en términos de AUC-ROC y precisión general.\n",
    "- Si el objetivo es **máxima discriminación entre clases (AUC-ROC)**, el modelo de Grand Slams con Gradient Boosting es el mejor. Si se prioriza **recall o F1**, el modelo MLP conjunto es más potente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c65aa-758a-47fc-9bc2-f511cbf15c5e",
   "metadata": {},
   "source": [
    "# Comparación entre modelos entrenados con diferentes datasets\n",
    "# Modelo Conjunto (GS + M1000) o Modelo GrandSlam\n",
    "\n",
    "En este apartado se evaluará si conviene entrenar modelos específicos por tipo de torneo (como Grand Slams) o si un modelo conjunto entrenado con torneos de tipo **Grand Slam + Masters 1000** puede generalizar mejor a los partidos de alto nivel.\n",
    "\n",
    "####  Objetivo\n",
    "Comparar el rendimiento de dos modelos de **Gradient Boosting (Optuna)** sobre el mismo conjunto de test: **partidos de Grand Slams**.\n",
    "\n",
    "- 🔹 **Modelo 1**: Entrenado exclusivamente con datos de torneos **Grand Slam**.\n",
    "- 🔹 **Modelo 2**: Entrenado con partidos de **Grand Slam + Masters 1000**.\n",
    "\n",
    "Ambos modelos han sido optimizados con **Optuna** y sus hiperparámetros fueron ajustados para maximizar el AUC-ROC.\n",
    "\n",
    "####  Proceso\n",
    "1. Se filtra el dataset para quedarnos únicamente con los partidos de **Grand Slams**.\n",
    "2. Se utilizan ambos modelos ya entrenados para predecir sobre ese conjunto.\n",
    "3. Se comparan las métricas :  \n",
    "   - Accuracy  \n",
    "   - Balanced Accuracy  \n",
    "   - Precision  \n",
    "   - Recall  \n",
    "   - F1-Score  \n",
    "   - AUC-ROC  \n",
    "   - Log Loss  \n",
    "\n",
    "Este análisis permite responder la siguiente pregunta clave del proyecto:\n",
    "\n",
    "> ¿Es preferible entrenar modelos específicos por tipo de torneo o uno conjunto para todos?\n",
    "\n",
    "Un mejor rendimiento del modelo conjunto sobre los datos de Grand Slams indicaría una **mejor generalización** al beneficiarse de un mayor volumen y variedad de datos. Por el contrario, si el modelo entrenado solo con datos de Grand Slams ofrece mejores métricas, esto sugiere que la especialización por tipo de torneo puede ser más eficaz para predecir su resultado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f580e3-7251-44c8-a8f2-211fbc653e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 1: Cargar el dataset ===\n",
    "ruta_base = r\"./Modelo_Completo/columnas_añadidas\"\n",
    "nombre_archivo = \"escaladofinal.csv\"\n",
    "ruta_completa = os.path.join(ruta_base, nombre_archivo)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    ruta_completa,\n",
    "    delimiter=\";\",   # separador de columnas\n",
    "    decimal=\",\"      # separador decimal \n",
    ")\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "df = df.sort_values(by=\"Date\")\n",
    "\n",
    "# === Corregir comas decimales en las columnas numéricas ===\n",
    "features = [\n",
    "    \"Surface_WinRate_Favorite\", \"Surface_WinRate_Not_Favorite\",\n",
    "    \"Surface_Matches_Favorite\", \"Surface_Matches_Not_Favorite\",\n",
    "    \"WinStreak_Favorite\", \"WinStreak_Not_Favorite\", \"Win_Streak_Diff\",\n",
    "    \"Rank_Favorite\", \"Rank_Not_Favorite\", \"Rank_Diff_Signed\", \"Rank_Diff_Abs\",\n",
    "    \"Masters1000_Favorite\", \"Masters1000_Not_Favorite\",\n",
    "    \"GrandSlams_Favorite\", \"GrandSlams_Not_Favorite\"\n",
    "]\n",
    "\n",
    "for col in features:\n",
    "    if df[col].dtype == object:\n",
    "        df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "\n",
    "# === Separar datos ===\n",
    "df_gs = df[df[\"Series\"] == \"Grand Slam\"].copy()\n",
    "df_comb = df[df[\"Series\"].isin([\"Grand Slam\", \"Masters 1000\"])].copy()\n",
    "\n",
    "X_gs = df_gs[features]\n",
    "y_gs = df_gs[\"Favorite_Wins\"]\n",
    "X_comb = df_comb[features]\n",
    "y_comb = df_comb[\"Favorite_Wins\"]\n",
    "\n",
    "# === Entrenar modelo combinado (fuera del loop para acelerar) ===\n",
    "w_comb = compute_sample_weight(class_weight='balanced', y=y_comb)\n",
    "modelo_combinado = GradientBoostingClassifier(\n",
    "    learning_rate=0.014538386775171604,\n",
    "    n_estimators=475,\n",
    "    max_depth=4,\n",
    "    subsample=0.5162888716243986,\n",
    "    random_state=42\n",
    ")\n",
    "modelo_combinado.fit(X_comb, y_comb, sample_weight=w_comb)\n",
    "\n",
    "# === Validación cruzada solo en partidos de GS ===\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_pred_gs = np.zeros(len(y_gs))\n",
    "y_proba_gs = np.zeros(len(y_gs))\n",
    "y_pred_comb = np.zeros(len(y_gs))\n",
    "y_proba_comb = np.zeros(len(y_gs))\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_gs, y_gs):\n",
    "    X_train, X_test = X_gs.iloc[train_idx], X_gs.iloc[test_idx]\n",
    "    y_train, y_test = y_gs.iloc[train_idx], y_gs.iloc[test_idx]\n",
    "    w = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "    modelo_gs = GradientBoostingClassifier(\n",
    "        learning_rate=0.0222992728201512,\n",
    "        n_estimators=484,\n",
    "        max_depth=3,\n",
    "        subsample=0.6374765489547842,\n",
    "        random_state=42\n",
    "    )\n",
    "    modelo_gs.fit(X_train, y_train, sample_weight=w)\n",
    "\n",
    "    y_pred_gs[test_idx] = modelo_gs.predict(X_test)\n",
    "    y_proba_gs[test_idx] = modelo_gs.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    y_pred_comb[test_idx] = modelo_combinado.predict(X_test)\n",
    "    y_proba_comb[test_idx] = modelo_combinado.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Función de evaluación ===\n",
    "def evaluar(nombre, y_true, y_pred, y_proba):\n",
    "    print(f\"\\n--- {nombre} ---\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 3))\n",
    "    print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y_true, y_pred), 3))\n",
    "    print(\"Precision:\", round(precision_score(y_true, y_pred), 3))\n",
    "    print(\"Recall:\", round(recall_score(y_true, y_pred), 3))\n",
    "    print(\"F1 Score:\", round(f1_score(y_true, y_pred), 3))\n",
    "    print(\"AUC-ROC:\", round(roc_auc_score(y_true, y_proba), 3))\n",
    "    print(\"Log Loss:\", round(log_loss(y_true, y_proba), 3))\n",
    "    print(\"Matriz de Confusión:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "# === Resultados ===\n",
    "y_true = y_gs.to_numpy()\n",
    "evaluar(\"Modelo SOLO Grand Slams\", y_true, y_pred_gs.astype(int), y_proba_gs)\n",
    "evaluar(\"Modelo COMBINADO (entrenado en GS + M1000)\", y_true, y_pred_comb.astype(int), y_proba_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2841a7-70e5-4a56-8594-6912c5dfc39a",
   "metadata": {},
   "source": [
    "###  Comparativa de Modelos: Evaluación sobre Partidos de Grand Slams\n",
    "\n",
    "| Métrica               | GB (Solo GS) | GB (GS + M1000) |\n",
    "|-----------------------|--------------|------------------|\n",
    "| **Accuracy**          | 0.641        | **0.669**        |\n",
    "| **Balanced Accuracy** | 0.656        | **0.685**        |\n",
    "| **Precision**         | 0.834        | **0.853**        |\n",
    "| **Recall**            | 0.620        | **0.648**        |\n",
    "| **F1-Score**          | 0.711        | **0.737**        |\n",
    "| **AUC-ROC**           | 0.713        | **0.752**        |\n",
    "| **Log Loss**          | 0.612        | **0.585**        |\n",
    "\n",
    "---\n",
    "\n",
    "###  Evaluación del modelo combinado sobre todos los torneos (GS + M1000)\n",
    "\n",
    "| Métrica               | Valor         |\n",
    "|-----------------------|---------------|\n",
    "| **Accuracy**          | 0.613         |\n",
    "| **Balanced Accuracy** | 0.631         |\n",
    "| **Precision**         | 0.792         |\n",
    "| **Recall**            | 0.581         |\n",
    "| **F1-Score**          | 0.670         |\n",
    "| **AUC-ROC**           | 0.682         |\n",
    "| **Log Loss**          | 0.636         |\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusiones sobre Grand Slams\n",
    "\n",
    "- El modelo **combinado (entrenado con Grand Slams + Masters 1000)** mejora claramente en todos los aspectos cuando se evalúa exclusivamente en partidos de Grand Slam:\n",
    "  - **Mejor discriminación (AUC-ROC: 0.752)**\n",
    "  - **Mayor precisión y recall**\n",
    "  - **Menor log loss**\n",
    "\n",
    "- Esto indica que los datos de Masters 1000 **aportan conocimiento útil que generaliza bien hacia los partidos de Grand Slam**, posiblemente porque:\n",
    "  - Ambos tipos de torneos son de alto nivel competitivo.\n",
    "  - Jugadores y condiciones similares permiten al modelo extraer patrones adicionales que no estaban presentes solo en los Grand Slams.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Por qué el modelo combinado rinde peor cuando se le evalúa sobre *todos los torneos*?\n",
    "\n",
    "- Cuando el modelo se evalúa sobre **el conjunto completo de partidos (GS + M1000)**, su rendimiento baja notablemente:\n",
    "  - **Accuracy pasa de 0.669 (en GS) a 0.613 (en total)**\n",
    "  - **AUC-ROC cae de 0.752 a 0.682**\n",
    "\n",
    "Esto puede deberse a:\n",
    "\n",
    "1. **Mayor variabilidad en los partidos de Masters 1000:**\n",
    "   - Estos torneos pueden incluir más sorpresas, rotaciones, lesiones, o jugadores menos constantes, lo que introduce más *ruido* en los datos.\n",
    "\n",
    "2. **Desbalance estructural más complejo:**\n",
    "   - El conjunto combinado tiene aún más partidos con el favorito ganando, lo que puede desestabilizar el equilibrio del modelo.\n",
    "\n",
    "3. **Contexto competitivo distinto:**\n",
    "   - Aunque ambos torneos son de nivel alto, **la motivación, duración, y preparación** de los jugadores puede variar. En Grand Slams hay más al mejor de 5 sets, lo que tiende a beneficiar al favorito.\n",
    "\n",
    "4. **Dificultad del problema:**\n",
    "   - Predecir correctamente todos los torneos a la vez implica aprender a manejar **más heterogeneidad** (distintas superficies, niveles de presión, jugadores en forma o en baja), lo cual es más difícil.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión Final\n",
    "\n",
    "> **Entrenar con datos de Masters 1000 ayuda al modelo a predecir mejor los Grand Slams**, pero esa ganancia **no se transfiere al predecir ambos tipos de torneo simultáneamente**, donde el modelo enfrenta mayor complejidad y menor consistencia en los patrones.\n",
    "\n",
    "Esto resalta la importancia de:\n",
    "- Evaluar los modelos según el objetivo específico.\n",
    "- No asumir que más datos siempre implican mejor rendimiento en todos los escenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc807239-9b56-4224-9243-e56dcc559756",
   "metadata": {},
   "source": [
    "# Comparación entre modelos entrenados con diferentes datasets  \n",
    "## Modelo Conjunto (GS + M1000) vs. Modelo Masters 1000\n",
    "\n",
    "En este apartado se evaluará si conviene entrenar modelos específicos por tipo de torneo (como los **Masters 1000**) o si un modelo conjunto entrenado con torneos de tipo **Grand Slam + Masters 1000** puede generalizar mejor a los partidos de alto nivel.\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivo\n",
    "Comparar el rendimiento de dos modelos de **Gradient Boosting (Optuna)** sobre el mismo conjunto de test: **partidos de Masters 1000**.\n",
    "\n",
    "- 🔹 **Modelo 1**: Entrenado exclusivamente con datos de torneos **Masters 1000**.  \n",
    "- 🔹 **Modelo 2**: Entrenado con partidos de **Masters 1000 + Grand Slam**.\n",
    "\n",
    "Ambos modelos han sido optimizados con **Optuna** y sus hiperparámetros fueron ajustados específicamente para maximizar el rendimiento en sus datasets respectivos.\n",
    "\n",
    "---\n",
    "\n",
    "### Proceso\n",
    "1. Se filtra el dataset para quedarnos únicamente con los partidos de **Masters 1000**.\n",
    "2. Se realiza validación cruzada estratificada para evaluar ambos modelos exclusivamente sobre estos partidos.\n",
    "3. Se comparan las siguientes métricas:\n",
    "   - Accuracy  \n",
    "   - Balanced Accuracy  \n",
    "   - Precision  \n",
    "   - Recall  \n",
    "   - F1-Score  \n",
    "   - AUC-ROC  \n",
    "   - Log Loss  \n",
    "\n",
    "Este análisis permite responder la siguiente pregunta clave del proyecto:\n",
    "\n",
    "> ¿Es preferible entrenar modelos específicos para torneos Masters 1000, o un modelo conjunto con más variedad de datos mejora la predicción?\n",
    "\n",
    "Un mejor rendimiento del modelo conjunto sobre los datos de Masters 1000 indicaría una **mejor capacidad de generalización**, posiblemente debido al uso de una mayor diversidad de ejemplos competitivos. Por el contrario, si el modelo entrenado exclusivamente con datos de Masters 1000 ofrece mejores resultados, esto sugiere que la especialización por tipo de torneo puede ser más eficaz para capturar patrones propios de estos eventos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fb1d1-ef45-4776-9032-7aa3718bb9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Separar datasets \n",
    "df_m1000 = df[df[\"Series\"] == \"Masters 1000\"].copy()\n",
    "df_combined = df[df[\"Series\"].isin([\"Grand Slam\", \"Masters 1000\"])].copy()\n",
    "\n",
    "X_m1000 = df_m1000[features]\n",
    "y_m1000 = df_m1000[\"Favorite_Wins\"]\n",
    "X_comb = df_combined[features]\n",
    "y_comb = df_combined[\"Favorite_Wins\"]\n",
    "\n",
    "# === Modelo combinado (entrenado una vez) ===\n",
    "w_comb = compute_sample_weight(class_weight='balanced', y=y_comb)\n",
    "modelo_combinado = GradientBoostingClassifier(\n",
    "    learning_rate=0.014538386775171604,\n",
    "    n_estimators=475,\n",
    "    max_depth=4,\n",
    "    subsample=0.5162888716243986,\n",
    "    random_state=42\n",
    ")\n",
    "modelo_combinado.fit(X_comb, y_comb, sample_weight=w_comb)\n",
    "\n",
    "#  Validación cruzada para evaluación en M1000 \n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_pred_m1000 = np.zeros(len(y_m1000))\n",
    "y_proba_m1000 = np.zeros(len(y_m1000))\n",
    "y_pred_comb = np.zeros(len(y_m1000))\n",
    "y_proba_comb = np.zeros(len(y_m1000))\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_m1000, y_m1000):\n",
    "    X_train, X_test = X_m1000.iloc[train_idx], X_m1000.iloc[test_idx]\n",
    "    y_train, y_test = y_m1000.iloc[train_idx], y_m1000.iloc[test_idx]\n",
    "    w_m1000 = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "    modelo_m1000 = GradientBoostingClassifier(\n",
    "        learning_rate=0.011215234584834815,\n",
    "        n_estimators=392,\n",
    "        max_depth=4,\n",
    "        subsample=0.7922129095414916,\n",
    "        random_state=42\n",
    "    )\n",
    "    modelo_m1000.fit(X_train, y_train, sample_weight=w_m1000)\n",
    "\n",
    "    y_pred_m1000[test_idx] = modelo_m1000.predict(X_test)\n",
    "    y_proba_m1000[test_idx] = modelo_m1000.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    y_pred_comb[test_idx] = modelo_combinado.predict(X_test)\n",
    "    y_proba_comb[test_idx] = modelo_combinado.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluación final\n",
    "def evaluar_modelo(nombre, y_true, y_pred, y_proba):\n",
    "    print(f\"\\n--- MÉTRICAS: {nombre} ---\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 3))\n",
    "    print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y_true, y_pred), 3))\n",
    "    print(\"Precision:\", round(precision_score(y_true, y_pred), 3))\n",
    "    print(\"Recall:\", round(recall_score(y_true, y_pred), 3))\n",
    "    print(\"F1-Score:\", round(f1_score(y_true, y_pred), 3))\n",
    "    print(\"AUC-ROC:\", round(roc_auc_score(y_true, y_proba), 3))\n",
    "    print(\"Log Loss:\", round(log_loss(y_true, y_proba), 3))\n",
    "    print(\"Matriz de Confusión:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "# === Resultados ===\n",
    "evaluar_modelo(\"Modelo SOLO Masters 1000\", y_m1000, y_pred_m1000.astype(int), y_proba_m1000)\n",
    "evaluar_modelo(\"Modelo COMBINADO (entrenado en GS + M1000)\", y_m1000, y_pred_comb.astype(int), y_proba_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ac8c7-2405-4325-a211-31b3668f3bc2",
   "metadata": {},
   "source": [
    "###  Comparativa de Modelos: Evaluación sobre Partidos de Masters 1000\n",
    "\n",
    "| Métrica               | GB (Solo M1000) | GB (GS + M1000) |\n",
    "|-----------------------|------------------|------------------|\n",
    "| **Accuracy**          | 0.598            | **0.605**        |\n",
    "| **Balanced Accuracy** | 0.612            | **0.634**        |\n",
    "| **Precision**         | 0.756            | **0.789**        |\n",
    "| **Recall**            | **0.568**        | 0.540            |\n",
    "| **F1-Score**          | **0.649**        | 0.641            |\n",
    "| **AUC-ROC**           | 0.650            | **0.694**        |\n",
    "| **Log Loss**          | 0.652            | **0.639**        |\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusiones sobre Masters 1000\n",
    "\n",
    "- El modelo **combinado (entrenado con Grand Slams + Masters 1000)** mejora en:\n",
    "  - **Precision (0.789 vs 0.756)**\n",
    "  - **Balanced Accuracy (0.634 vs 0.612)**\n",
    "  - **AUC-ROC (0.694 vs 0.650)**\n",
    "  - **Log Loss (0.639 vs 0.652)**\n",
    "\n",
    "  Lo cual sugiere una **mejor discriminación entre clases** y mayor robustez general en sus predicciones.\n",
    "\n",
    "- Por otro lado, el modelo **entrenado exclusivamente en Masters 1000** presenta:\n",
    "  - **Mejor recall (0.568 vs 0.540)**\n",
    "  - **Mejor F1-Score (0.649 vs 0.641)**\n",
    "\n",
    "  Esto indica que tiene **mejor capacidad para detectar victorias reales del favorito**, sacrificando algo de precisión.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión Final\n",
    "\n",
    "> Aunque el modelo especializado en Masters 1000 obtiene mejor **recall y F1**, el modelo conjunto ofrece **mejor discriminación general y precisión**.\n",
    "\n",
    "Esto sugiere que:\n",
    "- El modelo combinado es útil cuando se busca **consistencia general**.\n",
    "- El modelo específico de Masters 1000 puede ser preferido si se prioriza **capturar más victorias reales del favorito**, aunque eso implique más falsos positivos.\n",
    "\n",
    "Ambos enfoques tienen valor, y la elección entre ellos dependerá del **objetivo final del sistema de predicción**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
