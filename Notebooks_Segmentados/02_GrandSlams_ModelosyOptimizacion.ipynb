{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89cb7ee-29b9-48f9-b705-17a6ee6ab370",
   "metadata": {},
   "source": [
    "# Modelo Regresión Logistica GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5548c531-772e-41df-9ea5-510da369a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar los valores nulos por columna para comprobar que no haya nulos\n",
    "print(df[features_to_scale + [\"Favorite_Wins\"]].isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbae36f-fc78-4eb8-864e-b0da89f027c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features_to_scale]  # Asegurarse de que no haya columnas extra\n",
    "y = df[\"Favorite_Wins\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d9a7f-f5ed-4aa3-939f-e37d1208caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos que no haya nulos en x ni en y\n",
    "print(\"¿NaNs en X?:\", X.isna().any().any()) \n",
    "print(\"¿NaNs en y?:\", y.isna().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347e01f-8729-49c2-8911-f4aa29227dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo de regresión logística\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular y mostrar la métrica\n",
    "score = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", round(score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c60d44-9e92-4a61-9037-0f0dfb133beb",
   "metadata": {},
   "source": [
    "Que el resultado de Balanced Accuracy = 0.545 indica que el modelo está rindiendo solo ligeramente mejor que un clasificador aleatorio (que tendría una balanced accuracy de ~0.5 en un problema balanceado). Esto puede deberse a que las clases esten desbalanceadas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc6377-59b8-4161-8c24-d642be483927",
   "metadata": {},
   "source": [
    "verificamos cuántas veces gana el favorito (Favorite_Wins). Si hay muchas más 1s que 0s, la métrica accuracy o incluso balanced_accuracy puede estar sesgada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00889233-cfaa-47ae-9f29-609d2fd7d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c33b6d2-7001-4d41-86c0-2800799cda15",
   "metadata": {},
   "source": [
    "El favorito gana en el 71.3% de los casos (Favorite_Wins = 1)  \n",
    "\n",
    "El no favorito gana solo el 28.6% de las veces (Favorite_Wins = 0)  \n",
    "\n",
    "dataset está desbalanceado, y eso afecta directamente al rendimiento de modelos como regresión logística, que no manejan bien el desbalance sin ajustes. Un clasificador que siempre predice 1 tendría una accuracy del 71.3%, pero no estaría aprendiendo nada útil.\n",
    "\n",
    "\n",
    "Esto hace que el modelo penalice más los errores en la clase minoritaria (0), equilibrando el aprendizaje:\n",
    "\n",
    "Cuando usas un modelo como LogisticRegression sin el argumento class_weight='balanced', trata a todas las clases por igual, aunque tengas muchas más observaciones de una clase que de otra.\n",
    "\n",
    "Sin embargo, cuando los datos están desbalanceados (por ejemplo, 71.3% de favoritos ganan y solo 28.6% pierden), el modelo puede aprender a \"ir siempre a lo seguro\" y predecir siempre que el favorito gana. Esto te da una aparente alta accuracy, pero no está aprendiendo realmente.\n",
    "\n",
    "Sin balanced: accuracy puede ser buena, pero solo porque acierta muchos “gana el favorito”.\n",
    "\n",
    "Con balanced: el modelo acierta más veces cuando el favorito pierde, lo cual es mucho más difícil y valioso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3caa672-9971-4a38-bc3a-8775c7dd279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo de regresión logística con ajuste por desbalance\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar con balanced accuracy\n",
    "score = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", round(score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a72fa5-6f9e-4000-86d4-041695d78097",
   "metadata": {},
   "source": [
    "### Prueba con validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeafe6e-4fd5-497b-a8ea-0e4a978b8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos solo las columnas que queremos escalar\n",
    "X = df[features_to_scale]\n",
    "y = df[\"Favorite_Wins\"]  \n",
    "\n",
    "# Modelo con validación cruzada (5 folds)\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "y_pred_cv = cross_val_predict(model, X, y, cv=5)\n",
    "\n",
    "# Evaluación\n",
    "score_cv = balanced_accuracy_score(y, y_pred_cv)\n",
    "print(\"Balanced Accuracy (CV):\", round(score_cv, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4251b8-7d58-4c4a-aaa1-755b8e70c2bb",
   "metadata": {},
   "source": [
    "## Evaluación del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0824b307-bd0b-4afd-b415-34c7e4d2f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones con validación cruzada\n",
    "y_proba = cross_val_predict(model, X, y, cv=5, method='predict_proba')\n",
    "y_pred = (y_proba[:, 1] >= 0.5).astype(int)  # convertir probabilidades en clases\n",
    "\n",
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912e736-bbcd-4987-bfda-bc08ab7724a5",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159e4e8-c02d-449a-a703-30efd850ade8",
   "metadata": {},
   "source": [
    "###  **Métricas del Modelo**\n",
    "\n",
    "| Métrica               | Valor | ¿Qué significa?                                                                                                            |\n",
    "|-----------------------|-------|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Accuracy**          | 0.642 | El modelo acierta en el 64,2% de los casos totales. Puede estar sesgado si las clases están desbalanceadas.               |\n",
    "| **Balanced Accuracy** | 0.634 | La media de los aciertos por clase. Corrige el sesgo de clases desbalanceadas. Más fiable que accuracy en este caso.      |\n",
    "| **Precision**         | 0.809 | Cuando el modelo predice \"gana el favorito\", acierta el 80,9% de las veces.                                                |\n",
    "| **Recall**            | 0.652 | De todas las veces que el favorito gana, el modelo lo detecta el 65,2%.                                                    |\n",
    "| **F1-Score**          | 0.722 | Media armónica entre precisión y recall. Mide el equilibrio entre ambos.                                                   |\n",
    "| **AUC-ROC**           | 0.684 | Capacidad del modelo para distinguir entre ganadores y perdedores, usando probabilidades. Entre 0.5 (azar) y 1 (perfecto). |\n",
    "| **Log Loss**          | 0.643 | Penaliza errores de predicción probabilística. Cuanto más bajo, mejor.                                                     |\n",
    "\n",
    "---\n",
    "\n",
    "###  **Matriz de Confusión**\n",
    "\n",
    "|                         | Predicho: 0 | Predicho: 1 | Total Real |\n",
    "|-------------------------|-------------|-------------|------------|\n",
    "| **Real: 0** (pierde)    | 2264        | 1407        | 3671       |\n",
    "| **Real: 1** (gana)      | 3187        | 5969        | 9156       |\n",
    "\n",
    "-  **2264 veces** el modelo acertó al predecir que el favorito **no ganaba** (derrota).\n",
    "-  **5969 veces** acertó al predecir que el favorito **ganaba**.\n",
    "-  **1407 veces** predijo \"gana\" cuando perdió.\n",
    "-  **3187 veces** predijo \"pierde\" cuando ganó.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Classification Report**\n",
    "\n",
    "| Clase | Precision | Recall | F1-Score | Soporte | Interpretación |\n",
    "|-------|-----------|--------|----------|---------|----------------|\n",
    "| **0** (pierde el favorito) | 0.42 | 0.62 | 0.50 | 3671 | Buena capacidad para detectar derrotas. Aunque la precisión es baja, el recall es decente (detecta 6 de cada 10). |\n",
    "| **1** (gana el favorito)   | 0.81 | 0.65 | 0.72 | 9156 | Alta precisión (cuando predice \"gana\", suele acertar), aunque se le escapan algunas victorias reales. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1977c74-5af4-4e11-ac7f-f1725f26c680",
   "metadata": {},
   "source": [
    " ## Random Forest + validación cruzada GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d6771-3ca5-4aa4-bf71-36805f2bb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_rf = cross_val_predict(rf_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_rf = (y_proba_rf[:, 1] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbdd227-4ccb-47ae-a125-c63eb7dacf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_rf), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_rf), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_rf), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_rf), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_rf), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_rf[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_rf[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_rf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291f167-3f3c-41c6-a8ef-30015abf803b",
   "metadata": {},
   "source": [
    "###  Comparativa de Modelos: Regresión Logística vs Random Forest\n",
    "\n",
    "| Métrica               | Regresión Logística | Random Forest | Comentario |\n",
    "|-----------------------|---------------------|----------------|------------|\n",
    "| **Accuracy**          | 0.642               | 0.707          | RF tiene mejor rendimiento general. |\n",
    "| **Balanced Accuracy** | 0.634               | 0.554          | Logística más equilibrada entre clases. |\n",
    "| **Precision**         | 0.809               | 0.739          | Logística es más precisa al predecir victorias. |\n",
    "| **Recall**            | 0.652               | 0.912          | RF detecta muchas más victorias del favorito. |\n",
    "| **F1-Score**          | 0.722               | 0.816          | RF logra mejor equilibrio entre precisión y recall. |\n",
    "| **AUC-ROC**           | 0.684               | 0.673          | Muy similares; capacidad discriminativa comparable. |\n",
    "| **Log Loss**          | 0.643               | 0.571          | RF produce mejores probabilidades calibradas. |\n",
    "\n",
    "---\n",
    "\n",
    "###  Classification Report del Modelo (Random Forest)\n",
    "\n",
    "| Clase | Precision | Recall | F1-Score | Soporte | Interpretación |\n",
    "|-------|-----------|--------|----------|---------|----------------|\n",
    "| **0** (pierde el favorito) | 0.47 | 0.20 | 0.28 | 3671 | Predice derrotas con precisión moderada, pero **solo detecta el 20%** de las reales. Bajo recall. |\n",
    "| **1** (gana el favorito)   | 0.74 | 0.91 | 0.82 | 9156 | Excelente rendimiento: **detecta el 91%** de las victorias y con buena precisión. |\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusiones:\n",
    "\n",
    "-  **Random Forest** mejora el rendimiento general, sobre todo en la **detección de victorias del favorito** (recall 91,2%).\n",
    "- Sin embargo, **sacrifica rendimiento en la clase minoritaria** (derrotas), con un recall del 20% y balanced accuracy inferior.\n",
    "-  La **regresión logística** es más equilibrada entre clases, pero menos potente globalmente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515dee15-430f-40b3-95fc-8e31f3af72f4",
   "metadata": {},
   "source": [
    "## Modelo Gradient Boosting GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a07666-0290-4664-8d6c-d48fc314e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_gb = cross_val_predict(gb_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_gb = (y_proba_gb[:, 1] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7b051-3a6a-46ca-acda-45c2df126120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_gb), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_gb), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_gb), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_gb[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_gb[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_gb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ccdda-87eb-4d0f-bad2-dd8ea50dea5d",
   "metadata": {},
   "source": [
    "###  Comparativa de Modelos: Regresión Logística vs Random Forest vs Gradient Boosting\n",
    "\n",
    "| Métrica               | Regresión Logística | Random Forest | Gradient Boosting | Comentario |\n",
    "|-----------------------|---------------------|----------------|-------------------|------------|\n",
    "| **Accuracy**          | 0.642               | 0.707          | 0.717             | GB y RF superan claramente a Logística. |\n",
    "| **Balanced Accuracy** | 0.634               | 0.554          | 0.553             | Logística es la más equilibrada entre clases. |\n",
    "| **Precision**         | 0.809               | 0.739          | 0.738             | Logística es más precisa al predecir victorias. |\n",
    "| **Recall**            | 0.652               | 0.912          | 0.936             | GB detecta la mayor parte de las victorias. |\n",
    "| **F1-Score**          | 0.722               | 0.816          | 0.825             | GB logra el mejor equilibrio entre precisión y recall. |\n",
    "| **AUC-ROC**           | 0.684               | 0.673          | 0.699             | GB tiene la mejor capacidad discriminativa. |\n",
    "| **Log Loss**          | 0.643               | 0.571          | 0.547             | GB también ofrece las mejores probabilidades calibradas. |\n",
    "\n",
    "---\n",
    "\n",
    "###  Classification Report del Modelo (Gradient Boosting)\n",
    "\n",
    "| Clase | Precision | Recall | F1-Score | Soporte | Interpretación |\n",
    "|-------|-----------|--------|----------|---------|----------------|\n",
    "| **0** (pierde el favorito) | 0.51 | 0.17 | 0.26 | 3671 | Muy baja capacidad para detectar derrotas. Solo identifica el 17% de ellas, con precisión limitada. |\n",
    "| **1** (gana el favorito)   | 0.74 | 0.94 | 0.83 | 9156 | Excelente rendimiento: detecta el 94% de las victorias con buena precisión. |\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusiones Generales\n",
    "\n",
    "- **Regresión Logística**: Modelo más equilibrado entre clases, pero con menor rendimiento global.\n",
    "- **Random Forest**: Muy buen desempeño para victorias del favorito, aunque limitado en la detección de derrotas.\n",
    "- **Gradient Boosting**: Mejor modelo en métricas globales (F1, AUC, Log Loss), pero **sacrifica notablemente la clase minoritaria** (recall del 17% en derrotas).\n",
    "- Si el objetivo es **predecir victorias del favorito con alta fiabilidad**, **Gradient Boosting es el modelo más eficaz**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75038239-81a6-4897-8300-db99b179c7e0",
   "metadata": {},
   "source": [
    "## Modelo Gradient Boosting balanceado GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75e400-8bc9-4c4d-835e-aa1dbe90b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = HistGradientBoostingClassifier(class_weight='balanced', max_iter=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_gb = cross_val_predict(gb_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_gb = (y_proba_gb[:, 1] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d42f7-6e4c-4776-b53f-8119f1a6f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_gb), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_gb), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_gb), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_gb[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_gb[:, 1]), 3))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_gb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897cfdd-f4fc-4d12-93a5-782e261e05c5",
   "metadata": {},
   "source": [
    "### Resultados del Modelo: HistGradientBoostingClassifier con `class_weight='balanced'`\n",
    "\n",
    "| Métrica               | Valor  | Interpretación |\n",
    "|-----------------------|--------|----------------|\n",
    "| **Accuracy**          | 0.633  | Acierta en el 63,3% de los casos totales. Más equilibrado pero algo menos preciso en global. |\n",
    "| **Balanced Accuracy** | 0.646  | La mejor hasta ahora. Buen equilibrio entre clases, útil si interesan las derrotas del favorito. |\n",
    "| **Precision**         | 0.825  | Cuando predice que gana el favorito, acierta el 82,5% de las veces. Muy alta precisión. |\n",
    "| **Recall**            | 0.617  | Detecta el 61,7% de las victorias reales del favorito. |\n",
    "| **F1-Score**          | 0.706  | Buen equilibrio entre precisión y recall. Muy similar a regresión logística. |\n",
    "| **AUC-ROC**           | 0.695  | Alta capacidad para distinguir entre clases en base a probabilidades. |\n",
    "| **Log Loss**          | 0.618  | Mejores predicciones probabilísticas que la regresión logística, peor que Gradient Boosting sin balancear. |\n",
    "\n",
    "---\n",
    "\n",
    "###  Matriz de Confusión\n",
    "\n",
    "|                 | Predicho: 0 | Predicho: 1 |\n",
    "|-----------------|-------------|-------------|\n",
    "| **Real: 0**     | 2475        | 1196        |\n",
    "| **Real: 1**     | 3506        | 5650        |\n",
    "\n",
    "-  Acierta **2475** derrotas del favorito (mucho mejor que otros modelos).\n",
    "-  Acierta **5650** victorias del favorito.\n",
    "-  Falla **1196** veces diciendo que ganaría y no fue así.\n",
    "-  Falla **3506** veces diciendo que perdería y en realidad ganó.\n",
    "\n",
    "---\n",
    "\n",
    "###  Classification Report\n",
    "\n",
    "| Clase | Precision | Recall | F1-Score | Soporte | Interpretación |\n",
    "|-------|-----------|--------|----------|---------|----------------|\n",
    "| **0** (pierde el favorito) | 0.41 | 0.67 | 0.51 | 3671 | Mejora notable en detección de derrotas (67%). Aunque la precisión no es alta, acierta muchas de las derrotas reales. |\n",
    "| **1** (gana el favorito)   | 0.83 | 0.62 | 0.71 | 9156 | Muy buena precisión (83%). Aunque se le escapan algunas victorias, mantiene buen balance. |\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusión\n",
    "\n",
    "Este modelo logra el **mejor equilibrio entre clases** hasta el momento. Es ideal si te interesa **detectar sorpresas (derrotas del favorito)** sin sacrificar completamente el rendimiento global. Aunque pierde algo de recall en victorias, gana mucho en representar mejor ambas clases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b33d69-1284-4787-9ecd-a1f9b7d3f77a",
   "metadata": {},
   "source": [
    "## MLPClassifier GS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e641f-7874-489f-be96-78f94ed9f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo MLP\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),  # una capa oculta con 100 neuronas\n",
    "    max_iter=300,\n",
    "    alpha=1e-4,\n",
    "    solver='adam',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_mlp = cross_val_predict(mlp_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_mlp = (y_proba_mlp[:, 1] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1ad91-6ee3-4a64-9d63-5589ed331fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_mlp), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_mlp), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_mlp), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_mlp[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_mlp[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_mlp))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88d6d6-9e84-4369-9f4d-272e09f8f240",
   "metadata": {},
   "source": [
    "### Resultados del Modelo: MLPClassifier (Perceptrón Multicapa)\n",
    "\n",
    "| Métrica               | Valor  | Interpretación |\n",
    "|-----------------------|--------|----------------|\n",
    "| **Accuracy**          | 0.714  | Buen rendimiento general: acierta en el 71,4% de los casos. |\n",
    "| **Balanced Accuracy** | 0.556  | Bastante baja: el modelo favorece fuertemente la clase 1 (victorias del favorito). |\n",
    "| **Precision**         | 0.739  | Cuando predice que gana el favorito, acierta el 73,9% de las veces. |\n",
    "| **Recall**            | 0.925  | Excelente: detecta el 92,5% de las victorias reales. |\n",
    "| **F1-Score**          | 0.822  | Excelente equilibrio entre precisión y recall para la clase mayoritaria. |\n",
    "| **AUC-ROC**           | 0.699  | Muy buen poder discriminativo usando probabilidades. |\n",
    "| **Log Loss**          | 0.551  | Predicciones probabilísticas bastante buenas. |\n",
    "\n",
    "---\n",
    "\n",
    "###  Matriz de Confusión\n",
    "\n",
    "|                 | Predicho: 0 | Predicho: 1 |\n",
    "|-----------------|-------------|-------------|\n",
    "| **Real: 0**     | 688         | 2983        |\n",
    "| **Real: 1**     | 689         | 8467        |\n",
    "\n",
    "-  El modelo acierta **8467 victorias del favorito** (recall altísimo).\n",
    "-  Solo detecta **688 derrotas reales**, y se equivoca **2983 veces** prediciendo que ganaría cuando en realidad perdió.\n",
    "\n",
    "---\n",
    "\n",
    "###  Classification Report (Resumen)\n",
    "\n",
    "| Clase | Precision | Recall | F1-Score | Soporte |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| **0** (pierde el favorito) | 0.50 | 0.19 | 0.27 | 3671 |\n",
    "| **1** (gana el favorito)   | 0.74 | 0.92 | 0.82 | 9156 |\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusión\n",
    "\n",
    "El MLP muestra muy buen rendimiento al predecir victorias del favorito**, con alta precisión y recall, pero ignora en gran parte las derrotas (clase minoritaria).\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091d9aa-4316-4e23-ae0c-b79ac35bd570",
   "metadata": {},
   "source": [
    "# MLPClassifier con SMOTE GS\n",
    "Cuando entrenas un modelo con clases desbalanceadas, este suele ignorar la clase minoritaria (como las derrotas del favorito), porque aprende a predecir siempre la mayoritaria para maximizar el acierto global.\n",
    "SMOTE lo soluciona generando datos sintéticos (falsos pero realistas) de la clase minoritaria, haciendo que el modelo:\n",
    "- preste atención a esa clase,\n",
    "- aprenda patrones también cuando el favorito pierde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d2de4-b8e6-4ed2-a911-b3e9988c3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Aplicar SMOTE al conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 3. Entrenar MLP sobre datos balanceados\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "mlp_model.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# 4. Predecir sobre el test original\n",
    "y_pred_mlp = mlp_model.predict(X_test)\n",
    "y_proba_mlp = mlp_model.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db53139-7b21-43a8-bec3-d6d0084c1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_mlp), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y_test, y_pred_mlp), 3))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred_mlp), 3))\n",
    "print(\"Recall:\", round(recall_score(y_test, y_pred_mlp), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y_test, y_pred_mlp), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y_test, y_proba_mlp[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y_test, y_proba_mlp[:, 1]), 3))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_mlp))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35a0ed-6b3e-4d08-90ed-66b2ee6264e0",
   "metadata": {},
   "source": [
    "###  Resultados del Modelo: MLPClassifier con SMOTE\n",
    "\n",
    "| Métrica               | Valor  | Interpretación |\n",
    "|-----------------------|--------|----------------|\n",
    "| **Accuracy**          | 0.625  | Acierta en el 62,5% de los casos. Disminuye ligeramente frente al MLP sin balancear, pero mejora la detección de derrotas. |\n",
    "| **Balanced Accuracy** | 0.632  | Mucho mejor que el MLP sin balancear (0.556). Buen avance en el tratamiento de clases desbalanceadas. |\n",
    "| **Precision**         | 0.815  | Muy alta cuando predice victorias (clase 1). |\n",
    "| **Recall**            | 0.615  | Más bajo que antes en victorias, pero mejora mucho en derrotas. |\n",
    "| **F1-Score**          | 0.701  | Buen equilibrio entre precisión y recall. |\n",
    "| **AUC-ROC**           | 0.681  | Alta capacidad discriminativa usando probabilidades. |\n",
    "| **Log Loss**          | 0.638  | Predicciones probabilísticas razonablemente fiables. |\n",
    "\n",
    "---\n",
    "\n",
    "###  Matriz de Confusión\n",
    "\n",
    "|                 | Predicho: 0 | Predicho: 1 |\n",
    "|-----------------|-------------|-------------|\n",
    "| **Real: 0**     | 474         | 256         |\n",
    "| **Real: 1**     | 707         | 1129        |\n",
    "\n",
    "-  Detecta correctamente **474 derrotas del favorito** (recall clase 0: **65%**).\n",
    "-  Detecta correctamente **1129 victorias del favorito** (recall clase 1: **61%**).\n",
    "-  Se confunde más que antes, pero **recupera la clase minoritaria**, lo que muchos modelos anteriores no lograban.\n",
    "\n",
    "---\n",
    "\n",
    "###  Classification Report (Resumen)\n",
    "\n",
    "| Clase | Precision | Recall | F1-Score | Soporte |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| **0** (pierde el favorito) | 0.40 | 0.65 | 0.50 | 730 |\n",
    "| **1** (gana el favorito)   | 0.82 | 0.61 | 0.70 | 1836 |\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusión\n",
    "\n",
    "El MLP entrenado con **SMOTE** logra un balance mucho mejor entre clases, especialmente en detectar derrotas del favorito.\n",
    "\n",
    "-  Aunque su accuracy baja un poco, es uno de los mejores modelos para detectar sorpresas (cuando el favorito pierde).\n",
    "-  El coste es una ligera pérdida de precisión general, pero se compensa con una visión más equilibrada del problema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d365e-4420-4e8f-a4ea-276a3071e68b",
   "metadata": {},
   "source": [
    "# Comparación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abc7b7-3940-47bc-bc9d-7f7b1a853fec",
   "metadata": {},
   "source": [
    "###  Comparativa de Modelos: Rendimiento Global\n",
    "\n",
    "| Modelo                    | Accuracy | Balanced Acc | Precision | Recall | F1-Score | AUC-ROC | Log Loss | Comentario principal |\n",
    "|---------------------------|----------|---------------|-----------|--------|----------|---------|----------|------------------------|\n",
    "| **Regresión Logística**   | 0.642    | 0.634         | 0.809     | 0.652  | 0.722    | 0.684   | 0.643    | Buen equilibrio, sencillo y fiable |\n",
    "| **Random Forest**         | 0.707    | 0.554         | 0.739     | 0.912  | 0.816    | 0.673   | 0.571    | Muy sesgado hacia victorias |\n",
    "| **Gradient Boosting**     | 0.717    | 0.553         | 0.738     | 0.936  | 0.825    | 0.699   | 0.547    | Mejor F1, pero ignora derrotas |\n",
    "| **GB con Balanceo**       | 0.633    | 0.646         | 0.825     | 0.617  | 0.706    | 0.695   | 0.618    | Mejor balance entre clases |\n",
    "| **MLP (sin balancear)**   | 0.714    | 0.556         | 0.739     | 0.925  | 0.822    | 0.699   | 0.551    | Gran recall en victorias, malo en derrotas |\n",
    "| **MLP + SMOTE**           | 0.625    | 0.632         | 0.815     | 0.615  | 0.701    | 0.681   | 0.638    | Buen equilibrio, mejor para detectar derrotas |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2c82d-44c9-494b-b011-832a7095f909",
   "metadata": {},
   "source": [
    "**-  Mejor modelo balanceado:** GB con balanceo o MLP + SMOTE, si  interesa detectar **derrotas del favorito**. Ambos mejoran considerablemente la **Balanced Accuracy** y el **recall de la clase minoritaria**, a costa de algo de rendimiento global.\n",
    "\n",
    "**-Mejor en rendimiento general (F1 más alto):** Gradient Boosting sin balanceo, con un **F1-Score de 0.825** y **recall del 93,6%** para las victorias. Ideal si solo interesa acertar al máximo los casos comunes.\n",
    "\n",
    "**- Mejor AUC-ROC (discriminación probabilística):** Gradient Boosting sin balanceo (**0.699**) y MLP sin balancear (**0.699**) empatan como los más fiables en cuanto a la calidad de las probabilidades que asignan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0b80a-8437-4a13-8c40-47a0fc0e3674",
   "metadata": {},
   "source": [
    "# Favorite_Loses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e565cd-e00b-43d5-90f9-87cecd8e6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features_to_scale]  # Asegurarse de que no haya columnas extra\n",
    "y = df[\"Favorite_Loses\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11de054-e4e4-4795-946d-9efb27d78e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos que no haya nulos en x ni en y\n",
    "print(\"¿NaNs en X?:\", X.isna().any().any()) \n",
    "print(\"¿NaNs en y?:\", y.isna().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679de391-27f6-43d5-aa44-c6dadfe50fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo de regresión logística\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular y mostrar la métrica\n",
    "score = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1aa0ce-74ac-4e27-b88f-7c357db2a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f643d-50b5-48db-bfe4-3552cf04b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo de regresión logística con ajuste por desbalance\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar con balanced accuracy\n",
    "score = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", round(score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f60ed7-b369-4e28-9f5c-37c7c3c22205",
   "metadata": {},
   "source": [
    "## Prueba con validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6a741-c8c2-44e1-9c76-187739000965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos solo las columnas que queremos escalar\n",
    "X = df[features_to_scale]\n",
    "y = df[\"Favorite_Loses\"]  \n",
    "\n",
    "# Modelo con validación cruzada (5 folds)\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "y_pred_cv = cross_val_predict(model, X, y, cv=5)\n",
    "\n",
    "# Evaluación\n",
    "score_cv = balanced_accuracy_score(y, y_pred_cv)\n",
    "print(\"Balanced Accuracy (CV):\", round(score_cv, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174d780-14ea-4dc3-9ebb-90039363930b",
   "metadata": {},
   "source": [
    "## Evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb520a97-6c8e-4657-99bf-3aa0f74ddefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones con validación cruzada\n",
    "y_proba = cross_val_predict(model, X, y, cv=5, method='predict_proba')\n",
    "y_pred = (y_proba[:, 1] >= 0.5).astype(int)  # convertir probabilidades en clases\n",
    "\n",
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e9bb2-9b0b-48e0-a1ce-37b0d19b7bec",
   "metadata": {},
   "source": [
    " ## Random Forest + validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74db3c2-9b10-47ea-a14d-39004b2ee5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_rf = cross_val_predict(rf_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_rf = (y_proba_rf[:, 1] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee9c7c-cc0f-4fb1-9942-0950cb438e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_rf), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_rf), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_rf), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_rf), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_rf), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_rf[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_rf[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_rf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911cf4a-6e19-4e1e-9af6-af7f1269da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_gb = cross_val_predict(gb_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_gb = (y_proba_gb[:, 1] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a113981-52f7-4e63-8d8c-cb8296a3b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_gb), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_gb), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_gb), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_gb[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_gb[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_gb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c6ade-218e-4b49-a702-98be92285c8d",
   "metadata": {},
   "source": [
    "\n",
    "## Modelo Gradient Boosting balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd280e-b243-443e-a93e-294ce0bb0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = HistGradientBoostingClassifier(class_weight='balanced', max_iter=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_gb = cross_val_predict(gb_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_gb = (y_proba_gb[:, 1] >= 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e2632-ed68-412b-ac6a-7a950b5b8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_gb), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_gb), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_gb), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_gb[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_gb[:, 1]), 3))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_gb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231ad9f-0577-4974-b01f-d0953c7df764",
   "metadata": {},
   "source": [
    "## MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b49c6f-3330-4c89-8cc9-d23bcebba7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Modelo MLP\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),  # una capa oculta con 100 neuronas\n",
    "    max_iter=300,\n",
    "    alpha=1e-4,\n",
    "    solver='adam',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_mlp = cross_val_predict(mlp_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_mlp = (y_proba_mlp[:, 1] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4f57d-39da-41b4-9328-f36d3c5a9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_mlp), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_mlp), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_mlp), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_mlp[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_mlp[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_mlp))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a782027-a897-4112-ba86-d00283922bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# MLPClassifier con SMOTE\n",
    "\n",
    "# 1. Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Aplicar SMOTE al conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 3. Entrenar MLP sobre datos balanceados\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "mlp_model.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# 4. Predecir sobre el test original\n",
    "y_pred_mlp = mlp_model.predict(X_test)\n",
    "y_proba_mlp = mlp_model.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcac55-91d5-4265-82f2-2ae6c7cd61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_mlp), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y_test, y_pred_mlp), 3))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred_mlp), 3))\n",
    "print(\"Recall:\", round(recall_score(y_test, y_pred_mlp), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y_test, y_pred_mlp), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y_test, y_proba_mlp[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y_test, y_proba_mlp[:, 1]), 3))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_mlp))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b0f93-4e83-4adb-8655-a8d9db1f0448",
   "metadata": {},
   "source": [
    "##  Plan de Mejora de Modelos\n",
    "\n",
    "### Seleccionar los algoritmos más prometedores\n",
    "- Criterio de selección: **mayor AUC-ROC**\n",
    "- Modelos seleccionados:\n",
    "  - `Gradient Boosting` (sin balanceo)\n",
    "  - `MLPClassifier (sin balancear)`\n",
    "\n",
    "---\n",
    "\n",
    "### Optimización de hiperparámetros\n",
    "\n",
    "####  Objetivo:\n",
    "- Mejorar el rendimiento de los modelos elegidos sin modificar su estructura interna.\n",
    "\n",
    "####  Métodos que usaré:\n",
    "- `RandomizedSearchCV`: ideal para pruebas rápidas con espacio de búsqueda definido.\n",
    "- `Optuna`: optimización bayesiana, más inteligente y eficiente en exploración de hiperparámetros.\n",
    "\n",
    "#### Parámetros a optimizar:\n",
    "\n",
    "**Gradient Boosting:**\n",
    "- `learning_rate`, `max_depth`, `n_estimators`, `subsample`\n",
    "\n",
    "**MLPClassifier:**\n",
    "- `hidden_layer_sizes`, `alpha`, `learning_rate_init`, `solver`, `activation`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0ac70-888f-4cd4-9e71-10b8aaf8a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features_to_scale]  # Asegurarse de que no haya columnas extra\n",
    "y = df[\"Favorite_Wins\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9086e2-cf4b-4db1-9aa5-9ba676669ec5",
   "metadata": {},
   "source": [
    "# Modelo Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a95973-aedf-4ed2-ac97-493d60133c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_gb = cross_val_predict(gb_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_gb = (y_proba_gb[:, 1] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c922f7f-6235-4434-852c-4b59d292c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_gb), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_gb), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_gb), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_gb[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_gb[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_gb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d846a-c837-4ff1-9b3e-54972307edba",
   "metadata": {},
   "source": [
    "# Modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8120f5-6717-40cb-a5d2-01fe1eaedcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo MLP\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),  # una capa oculta con 100 neuronas\n",
    "    max_iter=300,\n",
    "    alpha=1e-4,\n",
    "    solver='adam',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Validación cruzada\n",
    "y_proba_mlp = cross_val_predict(mlp_model, X, y, cv=5, method='predict_proba')\n",
    "y_pred_mlp = (y_proba_mlp[:, 1] >= 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9de8d-76a5-4585-bf9f-c69fb481ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Métricas\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_mlp), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_mlp), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_mlp), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_mlp[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_mlp[:, 1]), 3))\n",
    "\n",
    "# Matriz de confusión e informe\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred_mlp))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308332f6-be50-409f-b0ef-a67fdfca1e7e",
   "metadata": {},
   "source": [
    "## Mejora Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a6505-02bb-47f0-8c73-66241ca9d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definir el espacio de búsqueda de hiperparámetros\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# 2. Instanciar el modelo base\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 3. Búsqueda aleatoria con validación cruzada\n",
    "random_search = RandomizedSearchCV(\n",
    "    gb_model, param_dist, n_iter=20, cv=5,\n",
    "    scoring='roc_auc', random_state=42, n_jobs=-1\n",
    ")\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# 4. Mejor modelo encontrado\n",
    "best_gb = random_search.best_estimator_\n",
    "print(\"Mejores hiperparámetros encontrados:\", random_search.best_params_)\n",
    "\n",
    "# 5. Validación cruzada con el mejor modelo\n",
    "y_proba_gb = cross_val_predict(best_gb, X, y, cv=5, method='predict_proba')\n",
    "y_pred_gb = (y_proba_gb[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# 6. Evaluación\n",
    "print(\"\\nMétricas:\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_gb), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_gb), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_gb), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_gb), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_gb[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_gb[:, 1]), 3))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\\n\", confusion_matrix(y, y_pred_gb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a3521-d9a5-44ef-b454-81f5529b0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instanciar modelo con mejores hiperparámetros\n",
    "model = GradientBoostingClassifier(\n",
    "    subsample=0.6,\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Validación cruzada con probabilidad\n",
    "y_proba = cross_val_predict(model, X, y, cv=5, method='predict_proba', n_jobs=-1)\n",
    "y_pred = (y_proba[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# 3. Métricas\n",
    "print(\"MÉTRICAS DEL MODELO OPTIMIZADO (Gradient Boosting):\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba[:, 1]), 3))\n",
    "\n",
    "# 4. Matriz de Confusión\n",
    "print(\"\\nMatriz de Confusión:\\n\", confusion_matrix(y, y_pred))\n",
    "\n",
    "# 5. Reporte de Clasificación\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2385b-98e5-4df7-8de7-546fbf062048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Inicializar predicciones vacías\n",
    "y_proba = np.zeros((len(y), 2))\n",
    "y_pred = np.zeros(len(y))\n",
    "\n",
    "# 2. Validación cruzada manual con sample_weight\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    # Usamos .iloc para indexar por posición en Pandas\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Calcular pesos balanceados solo para entrenamiento\n",
    "    weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "    # Modelo Gradient Boosting optimizado\n",
    "    model = GradientBoostingClassifier(\n",
    "        subsample=0.6,\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.01,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entrenar con pesos\n",
    "    model.fit(X_train, y_train, sample_weight=weights)\n",
    "\n",
    "    # Guardar predicciones\n",
    "    y_proba[test_idx] = model.predict_proba(X_test)\n",
    "    y_pred[test_idx] = model.predict(X_test)\n",
    "\n",
    "# 3. Evaluación\n",
    "print(\"MÉTRICAS CON sample_weight='balanced':\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba[:, 1]), 3))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\\n\", confusion_matrix(y, y_pred.astype(int)))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred.astype(int)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31198b-6e8a-4ba6-a417-93c6270f5f85",
   "metadata": {},
   "source": [
    "## Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d7d7f-363f-43c4-bc2c-cccdab9b1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetivo de optimización\n",
    "def objective(trial):\n",
    "    # Hiperparámetros a optimizar\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    # Validación cruzada\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "        model = GradientBoostingClassifier(\n",
    "            **params,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, sample_weight=weights)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "# Crear y ejecutar estudio\n",
    "study = optuna.create_study(direction='maximize', study_name='GB_AUC_Optimization')\n",
    "study.optimize(objective, n_trials=100, n_jobs=1)  # subir n_trials a 50-100 si quieres mejor resultado\n",
    "\n",
    "# Mostrar mejor resultado\n",
    "print(\" Mejor AUC-ROC:\", round(study.best_value, 4))\n",
    "print(\" Mejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ad465-4515-4e86-8210-dd512e31dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo con los mejores hiperparámetros encontrados\n",
    "final_model = GradientBoostingClassifier(\n",
    "    learning_rate=0.0222992728201512,\n",
    "    n_estimators=484,\n",
    "    max_depth=3,\n",
    "    subsample=0.6374765489547842,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Calcular pesos para clases desbalanceadas\n",
    "weights = compute_sample_weight(class_weight='balanced', y=y)\n",
    "\n",
    "# Entrenar con todos los datos\n",
    "final_model.fit(X, y, sample_weight=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c19a433-973e-41fc-ae59-e8fa75ee5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_proba = np.zeros(len(y))\n",
    "y_pred = np.zeros(len(y))\n",
    "\n",
    "# Validación cruzada manual\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    w = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "    model = GradientBoostingClassifier(\n",
    "        learning_rate=0.0222992728201512,\n",
    "        n_estimators=484,\n",
    "        max_depth=3,\n",
    "        subsample=0.6374765489547842,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, sample_weight=w)\n",
    "    y_proba[test_idx] = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred[test_idx] = model.predict(X_test)\n",
    "\n",
    "# Evaluación\n",
    "y_pred = y_pred.astype(int)\n",
    "print(\"MÉTRICAS DEL MODELO FINAL (Optuna ):\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba), 3))\n",
    "print(\"\\nMatriz de Confusión:\\n\", confusion_matrix(y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83261a4b-dac1-4936-b585-5c6e1da45481",
   "metadata": {},
   "source": [
    "### Mejora MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be6904-0ac7-4804-bccd-6382e69ff8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a escalar\n",
    "features_to_scale = [\n",
    "    \"Surface_WinRate_Favorite\", \"Surface_WinRate_Not_Favorite\",\n",
    "    \"Surface_Matches_Favorite\", \"Surface_Matches_Not_Favorite\",\n",
    "    \"WinStreak_Favorite\", \"WinStreak_Not_Favorite\", \"Win_Streak_Diff\",\n",
    "    \"Rank_Favorite\", \"Rank_Not_Favorite\", \"Rank_Diff_Signed\", \"Rank_Diff_Abs\",\n",
    "    \"GrandSlams_Favorite\", \"GrandSlams_Not_Favorite\"\n",
    "]\n",
    "\n",
    "# Asegurarse de que son numéricos\n",
    "for col in features_to_scale:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Escalado con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f52f98-b902-41e9-9162-b617f41a6934",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features_to_scale]  # Asegurarse de que no haya columnas extra\n",
    "y = df[\"Favorite_Wins\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209003df-1dc6-411a-bcac-3cde6f7cac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Espacio de búsqueda de hiperparámetros\n",
    "param_dist_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "    'alpha': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'learning_rate_init': [0.0005, 0.001, 0.01, 0.1],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "# 2. Modelo base\n",
    "mlp_model = MLPClassifier(max_iter=300, random_state=42)\n",
    "\n",
    "# 3. Búsqueda aleatoria con validación cruzada\n",
    "mlp_random_search = RandomizedSearchCV(\n",
    "    mlp_model,\n",
    "    param_distributions=param_dist_mlp,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp_random_search.fit(X, y)\n",
    "\n",
    "# 4. Mejor modelo encontrado\n",
    "best_mlp = mlp_random_search.best_estimator_\n",
    "print(\"Mejores hiperparámetros encontrados:\", mlp_random_search.best_params_)\n",
    "\n",
    "# 5. Validación cruzada con el mejor modelo\n",
    "y_proba_mlp = cross_val_predict(best_mlp, X, y, cv=5, method='predict_proba')\n",
    "y_pred_mlp = (y_proba_mlp[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# 6. Evaluación\n",
    "print(\"\\n MÉTRICAS MLPClassifier (RandomizedSearchCV):\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred_mlp), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred_mlp), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred_mlp), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred_mlp), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba_mlp[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba_mlp[:, 1]), 3))\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\\n\", confusion_matrix(y, y_pred_mlp))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00aede-3c34-4b93-91ef-0f28f685db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar progreso en consola\n",
    "#optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# Objetivo de optimización\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50, 50), (100, 50), (100, 100)]),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-5, 1e-2, log=True),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 0.0005, 0.1, log=True),\n",
    "        'solver': trial.suggest_categorical('solver', ['adam', 'sgd']),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    }\n",
    "\n",
    "    aucs = []\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = MLPClassifier(\n",
    "            **params,\n",
    "            max_iter=300,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "# Crear estudio y optimizar\n",
    "study = optuna.create_study(direction='maximize', study_name='MLP_AUC_Optimization')\n",
    "study.optimize(objective, n_trials=150, n_jobs=1)  \n",
    "\n",
    "# Mostrar mejores resultados\n",
    "print(\"Mejor AUC-ROC:\", round(study.best_value, 4))\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545eaff-2d23-42d0-819f-0a5a6183bc14",
   "metadata": {},
   "source": [
    " Mejor AUC-ROC: 0.7108\n",
    " Mejores hiperparámetros encontrados:\n",
    "{'hidden_layer_sizes': (50,), 'alpha': 1.1657970122657613e-05, 'learning_rate_init': 0.0005703910810991549, 'solver': 'adam', 'activation': 'relu'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a793be-e936-4a9e-a212-544db3ab9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modelo con los mejores hiperparámetros encontrados por Optuna\n",
    "best_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),\n",
    "    alpha=1.1657970122657613e-05,\n",
    "    learning_rate_init=0.0005703910810991549,\n",
    "    solver='adam',\n",
    "    activation='relu',\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Validación cruzada (probabilidades)\n",
    "y_proba = cross_val_predict(best_mlp, X, y, cv=5, method='predict_proba')\n",
    "y_pred = (y_proba[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# 3. Métricas\n",
    "print(\"MÉTRICAS DEL MLP OPTIMIZADO (Optuna final):\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y, y_pred), 3))\n",
    "print(\"Balanced Accuracy:\", round(balanced_accuracy_score(y, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y, y_pred), 3))\n",
    "print(\"Recall:\", round(recall_score(y, y_pred), 3))\n",
    "print(\"F1-Score:\", round(f1_score(y, y_pred), 3))\n",
    "print(\"AUC-ROC:\", round(roc_auc_score(y, y_proba[:, 1]), 3))\n",
    "print(\"Log Loss:\", round(log_loss(y, y_proba[:, 1]), 3))\n",
    "\n",
    "# 4. Matriz de confusión\n",
    "print(\"\\nMatriz de Confusión:\\n\", confusion_matrix(y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd14efd-7c8f-4488-9255-45a835d9af4d",
   "metadata": {},
   "source": [
    "### Comparativa completa de modelos (Gradient Boosting y MLP)\n",
    "\n",
    "| Métrica               | GB    Optuna | GB    RandomSearch | GB     Original | MLP     Optuna | MLP     RandomSearch | MLP     Original |\n",
    "|-----------------------|--------------|---------------------|----------------|----------------|----------------------|------------------|\n",
    "| **Accuracy**          | 0.643        | **0.718**           | 0.717          | **0.718**      | 0.714                | 0.714            |\n",
    "| **Balanced Accuracy** | **0.658**    | 0.530               | 0.553          | 0.557          | 0.556                | 0.556            |\n",
    "| **Precision**         | **0.835**    | 0.727               | 0.738          | 0.739          | **0.740**            | 0.739            |\n",
    "| **Recall**            | 0.623        | **0.969**           | **0.936**      | **0.935**      | 0.926                | 0.925            |\n",
    "| **F1-Score**          | 0.714        | **0.831**           | **0.825**      | **0.826**      | 0.822                | 0.822            |\n",
    "| **AUC-ROC**           | **0.713**    | 0.701               | 0.699          | 0.700          | 0.700                | 0.699            |\n",
    "| **Log Loss**          | 0.613        | **0.547**           | **0.547**      | **0.548**      | 0.551                | 0.551            |\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusiones\n",
    "\n",
    "- **Gradient Boosting**:\n",
    "  - Optuna logra el **mejor AUC-ROC** y balance entre clases.\n",
    "  -  RandomSearch logra el **mejor Accuracy, Recall y F1**, pero es más sesgado.\n",
    "  -  Original ya era sólido, pero ambos métodos  mejoran alguna métrica.  \n",
    "\n",
    "    \n",
    "---\n",
    "- **MLPClassifier**:\n",
    "  - Las diferencias entre versiones son **mínimas**.\n",
    "  -  Optuna mejora ligeramente en F1 y AUC-ROC.\n",
    "  -  RandomSearch tiene mejor Precision.\n",
    "  -  Original ya tenía un rendimiento alto, las mejoras son sutiles.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4a57f-608d-4379-b85b-885648818d1a",
   "metadata": {},
   "source": [
    "# Curva de precision y recall en función del umbral Gradien Boosting y MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df744f-3491-440f-ad6a-bc2aceedd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos optimizados\n",
    "best_gb = GradientBoostingClassifier(\n",
    "    learning_rate=0.0223,\n",
    "    n_estimators=484,\n",
    "    max_depth=3,\n",
    "    subsample=0.6375,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),\n",
    "    alpha=1.1658e-05,\n",
    "    learning_rate_init=0.00057,\n",
    "    solver='adam',\n",
    "    activation='relu',\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validation para obtener probabilidades\n",
    "y_scores_gb = cross_val_predict(best_gb, X, y, cv=5, method='predict_proba')[:, 1]\n",
    "y_scores_mlp = cross_val_predict(best_mlp, X, y, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# Calcular curvas\n",
    "precision_gb, recall_gb, thresholds_gb = precision_recall_curve(y, y_scores_gb)\n",
    "precision_mlp, recall_mlp, thresholds_mlp = precision_recall_curve(y, y_scores_mlp)\n",
    "\n",
    "# Grafico\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds_gb, precision_gb[:-1], label='Precision GB')\n",
    "plt.plot(thresholds_gb, recall_gb[:-1], label='Recall GB')\n",
    "\n",
    "plt.plot(thresholds_mlp, precision_mlp[:-1], label='Precision MLP', linestyle='--')\n",
    "plt.plot(thresholds_mlp, recall_mlp[:-1], label='Recall MLP', linestyle='--')\n",
    "\n",
    "plt.xlabel('Umbral de decisión')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Precision y Recall según umbral de decisión')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffa444-ebb0-4cce-a334-ad35077a3277",
   "metadata": {},
   "source": [
    "##  Análisis de Precision y Recall según el Umbral de Decisión\n",
    "\n",
    "La gráfica muestra cómo varían la **precisión** y el **recall** de los modelos optimizados (**Gradient Boosting** y **MLPClassifier**) en función del umbral de decisión aplicado a las probabilidades predichas.\n",
    "\n",
    "###  Objetivo de esta exploración\n",
    "\n",
    "- Evaluar cómo cambia el comportamiento del modelo cuando ajustamos el umbral de clasificación (por defecto es 0.5).\n",
    "- Identificar si hay un punto de equilibrio entre **precision** y **recall**.\n",
    "- Decidir si conviene modificar el umbral para mejorar el rendimiento según el objetivo del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "##  Observaciones clave de la gráfica\n",
    "\n",
    "- Cuando el **umbral es bajo** (cerca de 0.0), ambos modelos presentan:\n",
    "  - **Recall alto** (cerca de 1.0): detectan casi todos los positivos.\n",
    "  - **Precision baja**: hay muchos falsos positivos.\n",
    "\n",
    "- Cuando el **umbral sube** hacia 1.0:\n",
    "  - La **precisión aumenta**: los positivos predichos son más confiables.\n",
    "  - El **recall cae**: se pierden muchas verdaderas clases positivas.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  Conclusiones \n",
    "\n",
    "- La gráfica permite **ajustar el umbral de decisión de forma informada**: si el objetivo es **detectar con más seguridad cuándo gana el favorito** (más recall), se puede elegir un umbral bajo (ej. 0.4).\n",
    "- Si en cambio se quiere **ser más preciso** al decir que el favorito gana (menos falsos positivos), se puede optar por umbrales más altos (ej. 0.6).\n",
    "\n",
    "\n",
    "Segun el contexto se debe elegir un umbral u otro:\n",
    "- **Si es más grave predecir que ganará el favorito cuando no lo hace** → prioriza **precision**.\n",
    "- **Si es más importante detectar casi todas las veces que gana el favorito** → prioriza **recall**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
